{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mceval.utils import LabelledDataset, GenerationPipeline, PromptFormatter, LlamaGenerationPipeline\n",
    "import datasets\n",
    "from transformers import AutoTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %ls\n",
    "\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# results_path = 'mceval_run1'\n",
    "models = {'llama2-chat': 'meta-llama/Llama-2-7b-chat-hf', 'llama3-instruct': 'meta-llama/Meta-Llama-3-8B-Instruct'}\n",
    "datasets = ['harmless', 'dilemmas']\n",
    "prompt_var = ['standard', 'newline']\n",
    "runs = ['run3', 'run4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decode function\n",
    "# def decode_top_0(example: dict, tokenizer) -> dict:\n",
    "#     # takes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_cols = [\n",
    "    'is_A_standard_with_reasoning', 'is_A_standard_no_reasoning',\n",
    "    'is_A_newline_with_reasoning', 'is_A_newline_no_reasoning',\n",
    "    'is_other_standard_with_reasoning', 'is_other_standard_no_reasoning',\n",
    "    'is_other_newline_with_reasoning', 'is_other_newline_no_reasoning'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cols = [\n",
    "                    'model', 'dataset', 'run', \n",
    "                    'num_A_standard_prompt_with_reasoning', 'num_A_newline_prompt_with_reasoning', \n",
    "                    'num_A_standard_prompt_no_reasoning', 'num_A_newline_prompt_no_reasoning',\n",
    "                    'num_other_standard_prompt_with_reasoning', 'num_other_newline_prompt_with_reasoning',\n",
    "                    'num_other_standard_prompt_no_reasoning', 'num_other_newline_prompt_no_reasoning',\n",
    "                    'num_prompt_var_cause_flip_with_reasoning', \n",
    "                    'num_prompt_var_cause_flip_no_reasoning', \n",
    "                    'num_reasoning_cause_flip_standard_prompt',\n",
    "                    'num_reasoning_cause_flip_newline_prompt',\n",
    "                    'num_reasoning_changed_prompt_var_flip',\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token A: 319, token B: 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token A: 362, token B: 426\n",
      "             model   dataset   run num_A_standard_prompt_with_reasoning  \\\n",
      "0      llama2-chat  harmless  run3                                    3   \n",
      "1      llama2-chat  harmless  run4                                    3   \n",
      "2      llama2-chat  dilemmas  run3                                   22   \n",
      "3      llama2-chat  dilemmas  run4                                   22   \n",
      "4  llama3-instruct  harmless  run3                                   53   \n",
      "\n",
      "  num_A_newline_prompt_with_reasoning num_A_standard_prompt_no_reasoning  \\\n",
      "0                                   3                                  3   \n",
      "1                                   3                                  3   \n",
      "2                                  17                                 14   \n",
      "3                                  17                                 14   \n",
      "4                                  54                                 41   \n",
      "\n",
      "  num_A_newline_prompt_no_reasoning num_other_standard_prompt_with_reasoning  \\\n",
      "0                                 3                                       55   \n",
      "1                                 3                                       55   \n",
      "2                                15                                       73   \n",
      "3                                15                                       73   \n",
      "4                                38                                        0   \n",
      "\n",
      "  num_other_newline_prompt_with_reasoning  \\\n",
      "0                                      55   \n",
      "1                                      55   \n",
      "2                                      72   \n",
      "3                                      72   \n",
      "4                                       0   \n",
      "\n",
      "  num_other_standard_prompt_no_reasoning  \\\n",
      "0                                     55   \n",
      "1                                     55   \n",
      "2                                     77   \n",
      "3                                     77   \n",
      "4                                      0   \n",
      "\n",
      "  num_other_newline_prompt_no_reasoning  \\\n",
      "0                                    55   \n",
      "1                                    55   \n",
      "2                                    80   \n",
      "3                                    80   \n",
      "4                                     0   \n",
      "\n",
      "  num_prompt_var_cause_flip_with_reasoning  \\\n",
      "0                                        0   \n",
      "1                                        0   \n",
      "2                                       21   \n",
      "3                                       21   \n",
      "4                                        7   \n",
      "\n",
      "  num_prompt_var_cause_flip_no_reasoning  \\\n",
      "0                                      0   \n",
      "1                                      0   \n",
      "2                                      9   \n",
      "3                                      9   \n",
      "4                                      5   \n",
      "\n",
      "  num_reasoning_cause_flip_standard_prompt  \\\n",
      "0                                        0   \n",
      "1                                        0   \n",
      "2                                       20   \n",
      "3                                       20   \n",
      "4                                       14   \n",
      "\n",
      "  num_reasoning_cause_flip_newline_prompt  \\\n",
      "0                                       0   \n",
      "1                                       0   \n",
      "2                                      18   \n",
      "3                                      18   \n",
      "4                                      18   \n",
      "\n",
      "  num_reasoning_changed_prompt_var_flip  \n",
      "0                                     0  \n",
      "1                                     0  \n",
      "2                                    26  \n",
      "3                                    26  \n",
      "4                                    12  \n"
     ]
    }
   ],
   "source": [
    "# TODO change so that other is not assumed to be small\n",
    "\n",
    "# init results full with empty columns\n",
    "results_full = pd.DataFrame(\n",
    "    columns=results_cols\n",
    "    )\n",
    "for model_identifier in models.keys():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model_identifier])\n",
    "    # get token for 'A' and 'B'\n",
    "    token_A = tokenizer(' A', return_tensors='pt')['input_ids'][0][-1].item()\n",
    "    token_B = tokenizer(' B', return_tensors='pt')['input_ids'][0][-1].item()\n",
    "    print(f\"token A: {token_A}, token B: {token_B}\")\n",
    "    if not isinstance(token_A, int):\n",
    "        print('Token A is type:', type(token_A))\n",
    "        raise ValueError('Token A is not an integer')\n",
    "    if not isinstance(token_B, int):\n",
    "        print('Token B is type:', type(token_B))\n",
    "        raise ValueError('Token B is not an integer')\n",
    "        \n",
    "    for dataset in datasets:\n",
    "        for run in runs:\n",
    "            # if run == 'run1' and model_identifier == 'llama2-chat':\n",
    "            #     # special processing for run1 llama2-chat\n",
    "            #     results_path = f'old_results/{model_identifier}/{dataset}_decoded_answers_{prompt}.jsonl'\n",
    "            # else:\n",
    "            # init results df\n",
    "            \n",
    "            # TODO Maybe restructure the columns, a bit messy\n",
    "            results_temp = {}\n",
    "            \n",
    "            results_temp['model'] = model_identifier\n",
    "            results_temp['dataset'] = dataset\n",
    "            results_temp['run'] = run\n",
    "            \n",
    "            # load dataset\n",
    "            \n",
    "            standard_with_reasoning_path = f'mceval_{run}/{model_identifier}/scores_with_reasoning_{dataset}_standard.jsonl'\n",
    "            standard_no_reasoning_path = f'mceval_{run}/{model_identifier}/scores_without_reasoning_{dataset}_standard.jsonl'\n",
    "            newline_with_reasoning_path = f'mceval_{run}/{model_identifier}/scores_with_reasoning_{dataset}_newline.jsonl'\n",
    "            newline_no_reasoning_path = f'mceval_{run}/{model_identifier}/scores_without_reasoning_{dataset}_newline.jsonl'\n",
    "            \n",
    "            # load as df\n",
    "            # print(f\"loading {standard_with_reasoning_path}..\")\n",
    "            standard_with_reasoning = pd.read_json(standard_with_reasoning_path, lines=True, orient='records')\n",
    "            # print(f\"loaded {standard_with_reasoning_path}\")\n",
    "            standard_no_reasoning = pd.read_json(standard_no_reasoning_path, lines=True, orient='records')\n",
    "            # print(f\"loaded {standard_no_reasoning_path}\")\n",
    "            newline_with_reasoning = pd.read_json(newline_with_reasoning_path, lines=True, orient='records')\n",
    "            # print(f\"loaded {newline_with_reasoning_path}\")\n",
    "            newline_no_reasoning = pd.read_json(newline_no_reasoning_path, lines=True, orient='records')\n",
    "            # print(f\"loaded {newline_no_reasoning_path}\")\n",
    "            \n",
    "            id_colname = 'top_k_ids'\n",
    "            # top_id_is_A and other\n",
    "            comparison_df = pd.DataFrame(columns=comparison_cols)\n",
    "            comparison_df['is_A_standard_with_reasoning'] = standard_with_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            comparison_df['is_A_standard_no_reasoning'] = standard_no_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            comparison_df['is_A_newline_with_reasoning'] = newline_with_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            comparison_df['is_A_newline_no_reasoning'] = newline_no_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            comparison_df['is_other_standard_with_reasoning'] = standard_with_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            comparison_df['is_other_standard_no_reasoning'] = standard_no_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            comparison_df['is_other_newline_with_reasoning'] = newline_with_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            comparison_df['is_other_newline_no_reasoning'] = newline_no_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            \n",
    "            \n",
    "            # standard_with_reasoning['top_id_is_A'] = standard_with_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            # standard_no_reasoning['top_id_is_A'] = standard_no_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            # standard_with_reasoning['top_id_is_other'] = standard_with_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            # standard_no_reasoning['top_id_is_other'] = standard_no_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            \n",
    "            # newline_with_reasoning['top_id_is_A'] = newline_with_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            # newline_no_reasoning['top_id_is_A'] = newline_no_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            # newline_with_reasoning['top_id_is_other'] = newline_with_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            # newline_no_reasoning['top_id_is_other'] = newline_no_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            \n",
    "            # count number of token_A \n",
    "            results_temp['num_A_standard_prompt_with_reasoning'] = comparison_df['is_A_standard_with_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_A_standard_prompt_no_reasoning'] = comparison_df['is_A_standard_no_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_A_newline_prompt_with_reasoning'] = comparison_df['is_A_newline_with_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_A_newline_prompt_no_reasoning'] = comparison_df['is_A_newline_no_reasoning'].value_counts().get(True, 0)\n",
    "            \n",
    "            \n",
    "            # count number of other\n",
    "            results_temp['num_other_standard_prompt_with_reasoning'] = comparison_df['is_other_standard_with_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_other_standard_prompt_no_reasoning'] = comparison_df['is_other_standard_no_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_other_newline_prompt_with_reasoning'] = comparison_df['is_other_newline_with_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_other_newline_prompt_no_reasoning'] = comparison_df['is_other_newline_no_reasoning'].value_counts().get(True, 0)\n",
    "        \n",
    "            \n",
    "            # num diff in top_id_is_A (between with and without reasoning)\n",
    "            other_comparisons = pd.DataFrame()\n",
    "            other_comparisons['standard_flipped'] = comparison_df['is_A_standard_with_reasoning'] != comparison_df['is_A_standard_no_reasoning']\n",
    "            other_comparisons['newline_flipped'] = comparison_df['is_A_newline_with_reasoning'] != comparison_df['is_A_newline_no_reasoning']\n",
    "            # other_comparisons['standard_flipped'] = standard_with_reasoning['top_id_is_A'] != standard_no_reasoning['top_id_is_A']\n",
    "            # other_comparisons['newline_flipped'] = newline_with_reasoning['top_id_is_A'] != newline_no_reasoning['top_id_is_A']\n",
    "            \n",
    "            # num diff in top_id_is_A (between standard and newline)\n",
    "            other_comparisons['prompt_var_flip_with_reasoning'] = comparison_df['is_A_standard_with_reasoning'] != comparison_df['is_A_newline_with_reasoning']\n",
    "            other_comparisons['prompt_var_flip_no_reasoning'] = comparison_df['is_A_standard_no_reasoning'] != comparison_df['is_A_newline_no_reasoning']\n",
    "            other_comparisons['reasoning_cause_flip_standard_prompt'] = comparison_df['is_A_standard_with_reasoning'] != comparison_df['is_A_standard_no_reasoning']\n",
    "            other_comparisons['reasoning_cause_flip_newline_prompt'] = comparison_df['is_A_newline_with_reasoning'] != comparison_df['is_A_newline_no_reasoning']\n",
    "            # other_comparisons['prompt_var_flip_with_reasoning'] = standard_with_reasoning['top_id_is_A'] != newline_with_reasoning['top_id_is_A']\n",
    "            # other_comparisons['prompt_var_flip_no_reasoning'] = standard_no_reasoning['top_id_is_A'] != newline_no_reasoning['top_id_is_A']\n",
    "            # other_comparisons['reasoning_cause_flip_standard_prompt'] = standard_with_reasoning['top_id_is_A'] != standard_no_reasoning['top_id_is_A']\n",
    "            # other_comparisons['reasoning_cause_flip_newline_prompt'] = newline_with_reasoning['top_id_is_A'] != newline_no_reasoning['top_id_is_A']\n",
    "            \n",
    "            other_comparisons['reasoning_changed_prompt_var_flip'] = other_comparisons['standard_flipped'] != other_comparisons['newline_flipped']\n",
    "            \n",
    "            # add values to results df\n",
    "            results_temp['num_prompt_var_cause_flip_with_reasoning'] = other_comparisons['prompt_var_flip_with_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_prompt_var_cause_flip_no_reasoning'] = other_comparisons['prompt_var_flip_no_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_reasoning_cause_flip_standard_prompt'] = other_comparisons['reasoning_cause_flip_standard_prompt'].value_counts().get(True, 0)\n",
    "            results_temp['num_reasoning_cause_flip_newline_prompt'] = other_comparisons['reasoning_cause_flip_newline_prompt'].value_counts().get(True, 0)\n",
    "            results_temp['num_reasoning_changed_prompt_var_flip'] = other_comparisons['reasoning_changed_prompt_var_flip'].value_counts().get(True, 0)\n",
    "            \n",
    "            # cat to results_full\n",
    "            # print(results_temp.head())\n",
    "            # print(results_full.head())\n",
    "            row_df = pd.DataFrame([results_temp])\n",
    "\n",
    "            results_full = pd.concat([results_full, row_df], ignore_index=True)\n",
    "            \n",
    "            # print(results_temp.head())\n",
    "            # print(other_comparisons.head())\n",
    "            # print(comparison_df.head())\n",
    "\n",
    "print(results_full.head())\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results_full to jsonl\n",
    "results_full.to_json('results_full.jsonl', orient='records', lines=True)\n",
    "# write to csv\n",
    "results_full.to_csv('results_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, dataset, prompt_var):\n",
    "    scores_without_reasoning_path = f'{results_path}/{model}/{dataset}/scores_without_reasoning_{prompt_var}.jsonl'\n",
    "    scores_with_reasoning_path = f'{results_path}/{model}/{dataset}/scores_with_reasoning_{prompt_var}.jsonl'\n",
    "    scores_without_reasoning = pd.read_json(scores_without_reasoning_path, orient='records', lines=True)\n",
    "    scores_with_reasoning = pd.read_json(scores_with_reasoning_path, orient='records', lines=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([128000,    362]) tensor([128000,    426])\n"
     ]
    }
   ],
   "source": [
    "token_A = tokenizer(' A', return_tensors='pt')['input_ids'][0]\n",
    "token_B = tokenizer(' B', return_tensors='pt')['input_ids'][0]\n",
    "print(token_A, token_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# tokenizer decode 426\n",
    "B_tok_llama3 = tokenizer.decode(426)\n",
    "print(B_tok_llama3[1])\n",
    "\n",
    "# 362\n",
    "A_tok_llama3 = tokenizer.decode(362)\n",
    "print(len(A_tok_llama3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer for llama2\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', ',', 'MS', 'PA', '']\n"
     ]
    }
   ],
   "source": [
    "unexpected_token_ids = [29889, 29892, 4345, 7228, 29871]\n",
    "print([tokenizer.decode(token_id) for token_id in unexpected_token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redteam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
