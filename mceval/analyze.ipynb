{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mceval.utils import LabelledDataset, GenerationPipeline, PromptFormatter, LlamaGenerationPipeline\n",
    "import datasets\n",
    "from transformers import AutoTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis_mceval.ipynb                  n_shot_jailbreak.py\n",
      "archive_lm_eval.ipynb                  \u001b[0m\u001b[38;5;27mold results\u001b[0m/\n",
      "binary_evals_development.ipynb         one_sentence_reasoning_transcripts\n",
      "boolq.yaml                             \u001b[38;5;27m__pycache__\u001b[0m/\n",
      "\u001b[38;5;27mdilemmas\u001b[0m/                              results_full.csv\n",
      "eval_test.ipynb                        results_full.jsonl\n",
      "\u001b[38;5;27mfigs\u001b[0m/                                  \u001b[38;5;27mrun_results\u001b[0m/\n",
      "harmless_dataset_with_predictions.csv  \u001b[38;5;27msocial-chem-101\u001b[0m/\n",
      "hhh_with_CoT.ipynb                     \u001b[38;5;9msocial-chem-101.zip\u001b[0m\n",
      "jailbreak_demo.ipynb                   ten_tokens_reasoning_transcripts\n",
      "\u001b[38;5;27mmceval\u001b[0m/                                \u001b[38;5;27mtest_hhh_config\u001b[0m/\n",
      "\u001b[38;5;27mmceval_run1\u001b[0m/                           testing_mceval.ipynb\n",
      "\u001b[38;5;27mmceval_run2\u001b[0m/                           tests.py\n",
      "\u001b[38;5;27mmceval_run3\u001b[0m/                           test_testing_mceval.ipynb\n",
      "\u001b[38;5;27mmceval_run4\u001b[0m/                           \u001b[38;5;27mtrials\u001b[0m/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%ls\n",
    "\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# results_path = 'mceval_run1'\n",
    "models = {'llama2-chat': 'meta-llama/Llama-2-7b-chat-hf', 'llama3-instruct': 'meta-llama/Meta-Llama-3-8B-Instruct'}\n",
    "datasets = ['harmless', 'dilemmas']\n",
    "prompt_var = ['standard', 'newline']\n",
    "runs = ['run3', 'run4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decode function\n",
    "# def decode_top_0(example: dict, tokenizer) -> dict:\n",
    "#     # takes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_cols = [\n",
    "    'is_A_standard_with_reasoning', 'is_A_standard_no_reasoning',\n",
    "    'is_A_newline_with_reasoning', 'is_A_newline_no_reasoning',\n",
    "    'is_other_standard_with_reasoning', 'is_other_standard_no_reasoning',\n",
    "    'is_other_newline_with_reasoning', 'is_other_newline_no_reasoning'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cols = [\n",
    "                    'model', 'dataset', 'run', \n",
    "                    'num_A_standard_prompt_with_reasoning', 'num_A_newline_prompt_with_reasoning', \n",
    "                    'num_A_standard_prompt_no_reasoning', 'num_A_newline_prompt_no_reasoning',\n",
    "                    'num_other_standard_prompt_with_reasoning', 'num_other_newline_prompt_with_reasoning',\n",
    "                    'num_other_standard_prompt_no_reasoning', 'num_other_newline_prompt_no_reasoning',\n",
    "                    'num_prompt_var_cause_flip_with_reasoning', \n",
    "                    'num_prompt_var_cause_flip_no_reasoning', \n",
    "                    'num_reasoning_cause_flip_standard_prompt',\n",
    "                    'num_reasoning_cause_flip_newline_prompt',\n",
    "                    'num_reasoning_changed_prompt_var_flip',\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             model  ... num_reasoning_changed_prompt_var_flip\n",
      "0      llama2-chat  ...                                     0\n",
      "1      llama2-chat  ...                                     0\n",
      "2      llama2-chat  ...                                     1\n",
      "3      llama2-chat  ...                                     1\n",
      "4  llama3-instruct  ...                                     0\n",
      "\n",
      "[5 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# init results full with empty columns\n",
    "results_full = pd.DataFrame(\n",
    "    columns=results_cols\n",
    "    )\n",
    "for model_identifier in models.keys():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model_identifier])\n",
    "    # get token for 'A' and 'B'\n",
    "    token_A = tokenizer('A', return_tensors='pt')['input_ids'][0][0].item()\n",
    "    token_B = tokenizer('B', return_tensors='pt')['input_ids'][0][0].item()\n",
    "    if not isinstance(token_A, int):\n",
    "        print('Token A is type:', type(token_A))\n",
    "        raise ValueError('Token A is not an integer')\n",
    "    if not isinstance(token_B, int):\n",
    "        print('Token B is type:', type(token_B))\n",
    "        raise ValueError('Token B is not an integer')\n",
    "        \n",
    "    for dataset in datasets:\n",
    "        for run in runs:\n",
    "            # if run == 'run1' and model_identifier == 'llama2-chat':\n",
    "            #     # special processing for run1 llama2-chat\n",
    "            #     results_path = f'old_results/{model_identifier}/{dataset}_decoded_answers_{prompt}.jsonl'\n",
    "            # else:\n",
    "            # init results df\n",
    "            \n",
    "            # TODO Maybe restructure the columns, a bit messy\n",
    "            results_temp = {}\n",
    "            \n",
    "            results_temp['model'] = model_identifier\n",
    "            results_temp['dataset'] = dataset\n",
    "            results_temp['run'] = run\n",
    "            \n",
    "            # load dataset\n",
    "            \n",
    "            standard_with_reasoning_path = f'mceval_{run}/{model_identifier}/scores_with_reasoning_{dataset}_standard.jsonl'\n",
    "            standard_no_reasoning_path = f'mceval_{run}/{model_identifier}/scores_without_reasoning_{dataset}_standard.jsonl'\n",
    "            newline_with_reasoning_path = f'mceval_{run}/{model_identifier}/scores_with_reasoning_{dataset}_newline.jsonl'\n",
    "            newline_no_reasoning_path = f'mceval_{run}/{model_identifier}/scores_without_reasoning_{dataset}_newline.jsonl'\n",
    "            \n",
    "            # load as df\n",
    "            # print(f\"loading {standard_with_reasoning_path}..\")\n",
    "            standard_with_reasoning = pd.read_json(standard_with_reasoning_path, lines=True, orient='records')\n",
    "            # print(f\"loaded {standard_with_reasoning_path}\")\n",
    "            standard_no_reasoning = pd.read_json(standard_no_reasoning_path, lines=True, orient='records')\n",
    "            # print(f\"loaded {standard_no_reasoning_path}\")\n",
    "            newline_with_reasoning = pd.read_json(newline_with_reasoning_path, lines=True, orient='records')\n",
    "            # print(f\"loaded {newline_with_reasoning_path}\")\n",
    "            newline_no_reasoning = pd.read_json(newline_no_reasoning_path, lines=True, orient='records')\n",
    "            # print(f\"loaded {newline_no_reasoning_path}\")\n",
    "            \n",
    "            id_colname = 'top_k_ids'\n",
    "            # top_id_is_A and other\n",
    "            comparison_df = pd.DataFrame(columns=comparison_cols)\n",
    "            comparison_df['is_A_standard_with_reasoning'] = standard_with_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            comparison_df['is_A_standard_no_reasoning'] = standard_no_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            comparison_df['is_A_newline_with_reasoning'] = newline_with_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            comparison_df['is_A_newline_no_reasoning'] = newline_no_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            comparison_df['is_other_standard_with_reasoning'] = standard_with_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            comparison_df['is_other_standard_no_reasoning'] = standard_no_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            comparison_df['is_other_newline_with_reasoning'] = newline_with_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            comparison_df['is_other_newline_no_reasoning'] = newline_no_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            \n",
    "            \n",
    "            # standard_with_reasoning['top_id_is_A'] = standard_with_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            # standard_no_reasoning['top_id_is_A'] = standard_no_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            # standard_with_reasoning['top_id_is_other'] = standard_with_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            # standard_no_reasoning['top_id_is_other'] = standard_no_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            \n",
    "            # newline_with_reasoning['top_id_is_A'] = newline_with_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            # newline_no_reasoning['top_id_is_A'] = newline_no_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            # newline_with_reasoning['top_id_is_other'] = newline_with_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            # newline_no_reasoning['top_id_is_other'] = newline_no_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            \n",
    "            # count number of token_A \n",
    "            results_temp['num_A_standard_prompt_with_reasoning'] = comparison_df['is_A_standard_with_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_A_standard_prompt_no_reasoning'] = comparison_df['is_A_standard_no_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_A_newline_prompt_with_reasoning'] = comparison_df['is_A_newline_with_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_A_newline_prompt_no_reasoning'] = comparison_df['is_A_newline_no_reasoning'].value_counts().get(True, 0)\n",
    "            \n",
    "            \n",
    "            # count number of other\n",
    "            results_temp['num_other_standard_prompt_with_reasoning'] = comparison_df['is_other_standard_with_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_other_standard_prompt_no_reasoning'] = comparison_df['is_other_standard_no_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_other_newline_prompt_with_reasoning'] = comparison_df['is_other_newline_with_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_other_newline_prompt_no_reasoning'] = comparison_df['is_other_newline_no_reasoning'].value_counts().get(True, 0)\n",
    "        \n",
    "            \n",
    "            # num diff in top_id_is_A (between with and without reasoning)\n",
    "            other_comparisons = pd.DataFrame()\n",
    "            other_comparisons['standard_flipped'] = comparison_df['is_A_standard_with_reasoning'] != comparison_df['is_A_standard_no_reasoning']\n",
    "            other_comparisons['newline_flipped'] = comparison_df['is_A_newline_with_reasoning'] != comparison_df['is_A_newline_no_reasoning']\n",
    "            # other_comparisons['standard_flipped'] = standard_with_reasoning['top_id_is_A'] != standard_no_reasoning['top_id_is_A']\n",
    "            # other_comparisons['newline_flipped'] = newline_with_reasoning['top_id_is_A'] != newline_no_reasoning['top_id_is_A']\n",
    "            \n",
    "            # num diff in top_id_is_A (between standard and newline)\n",
    "            other_comparisons['prompt_var_flip_with_reasoning'] = comparison_df['is_A_standard_with_reasoning'] != comparison_df['is_A_newline_with_reasoning']\n",
    "            other_comparisons['prompt_var_flip_no_reasoning'] = comparison_df['is_A_standard_no_reasoning'] != comparison_df['is_A_newline_no_reasoning']\n",
    "            other_comparisons['reasoning_cause_flip_standard_prompt'] = comparison_df['is_A_standard_with_reasoning'] != comparison_df['is_A_standard_no_reasoning']\n",
    "            other_comparisons['reasoning_cause_flip_newline_prompt'] = comparison_df['is_A_newline_with_reasoning'] != comparison_df['is_A_newline_no_reasoning']\n",
    "            # other_comparisons['prompt_var_flip_with_reasoning'] = standard_with_reasoning['top_id_is_A'] != newline_with_reasoning['top_id_is_A']\n",
    "            # other_comparisons['prompt_var_flip_no_reasoning'] = standard_no_reasoning['top_id_is_A'] != newline_no_reasoning['top_id_is_A']\n",
    "            # other_comparisons['reasoning_cause_flip_standard_prompt'] = standard_with_reasoning['top_id_is_A'] != standard_no_reasoning['top_id_is_A']\n",
    "            # other_comparisons['reasoning_cause_flip_newline_prompt'] = newline_with_reasoning['top_id_is_A'] != newline_no_reasoning['top_id_is_A']\n",
    "            \n",
    "            other_comparisons['reasoning_changed_prompt_var_flip'] = other_comparisons['standard_flipped'] != other_comparisons['newline_flipped']\n",
    "            \n",
    "            # add values to results df\n",
    "            results_temp['num_prompt_var_cause_flip_with_reasoning'] = other_comparisons['prompt_var_flip_with_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_prompt_var_cause_flip_no_reasoning'] = other_comparisons['prompt_var_flip_no_reasoning'].value_counts().get(True, 0)\n",
    "            results_temp['num_reasoning_cause_flip_standard_prompt'] = other_comparisons['reasoning_cause_flip_standard_prompt'].value_counts().get(True, 0)\n",
    "            results_temp['num_reasoning_cause_flip_newline_prompt'] = other_comparisons['reasoning_cause_flip_newline_prompt'].value_counts().get(True, 0)\n",
    "            results_temp['num_reasoning_changed_prompt_var_flip'] = other_comparisons['reasoning_changed_prompt_var_flip'].value_counts().get(True, 0)\n",
    "            \n",
    "            # cat to results_full\n",
    "            # print(results_temp.head())\n",
    "            # print(results_full.head())\n",
    "            row_df = pd.DataFrame([results_temp])\n",
    "\n",
    "            results_full = pd.concat([results_full, row_df], ignore_index=True)\n",
    "            \n",
    "            # print(results_temp.head())\n",
    "            # print(other_comparisons.head())\n",
    "            # print(comparison_df.head())\n",
    "\n",
    "print(results_full.head())\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results_full to jsonl\n",
    "results_full.to_json('results_full.jsonl', orient='records', lines=True)\n",
    "# write to csv\n",
    "results_full.to_csv('results_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, dataset, prompt_var):\n",
    "    scores_without_reasoning_path = f'{results_path}/{model}/{dataset}/scores_without_reasoning_{prompt_var}.jsonl'\n",
    "    scores_with_reasoning_path = f'{results_path}/{model}/{dataset}/scores_with_reasoning_{prompt_var}.jsonl'\n",
    "    scores_without_reasoning = pd.read_json(scores_without_reasoning_path, orient='records', lines=True)\n",
    "    scores_with_reasoning = pd.read_json(scores_with_reasoning_path, orient='records', lines=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-redteam2",
   "language": "python",
   "name": "redteam2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
