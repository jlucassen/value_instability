{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LabelledDataset, GenerationPipeline, PromptFormatter, LlamaGenerationPipeline\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze.ipynb  generate.py  old_analyze.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
      "/data/joshua_clymer/owen-workspace/value_instability\n"
     ]
    }
   ],
   "source": [
    "%ls\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_path = 'mceval_run1'\n",
    "models = ['llama2-chat': 'meta-llama/Llama-2-7b-chat-hf', 'llama3-instruct': 'meta-llama/Meta-Llama-3-8B-Instruct']\n",
    "datasets = ['harmless', 'dilemmas']\n",
    "prompt_var = ['standard', 'newline']\n",
    "runs = ['run1', 'run2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decode function\n",
    "# def decode_top_0(example: dict, tokenizer) -> dict:\n",
    "#     # takes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract num A first choice and flip percentage\n",
    "for model_identifier in models.keys():\n",
    "    generation_pipeline = GenerationPipeline(model_identifier)\n",
    "    tokenizer = generation_pipeline.tokenizer\n",
    "    # get token for 'A' and 'B'\n",
    "    token_A = tokenizer('A', return_tensors='pt')['input_ids'][0][0]\n",
    "    token_B = tokenizer('B', return_tensors='pt')['input_ids'][0][0]\n",
    "    if not isinstance(token_A, int):\n",
    "        print('Token A is type:', type(token_A))\n",
    "        raise ValueError('Token A is not an integer')\n",
    "    if not isinstance(token_B, int):\n",
    "        print('Token B is type:', type(token_B))\n",
    "        raise ValueError('Token B is not an integer')\n",
    "        \n",
    "    for dataset in datasets:\n",
    "        for run in runs:\n",
    "            # if run == 'run1' and model_identifier == 'llama2-chat':\n",
    "            #     # special processing for run1 llama2-chat\n",
    "            #     results_path = f'old_results/{model_identifier}/{dataset}_decoded_answers_{prompt}.jsonl'\n",
    "            # else:\n",
    "            # init results df\n",
    "            \n",
    "            # TODO Maybe restructure the columns, a bit messy\n",
    "            results_values = pd.DataFrame(\n",
    "                columns=[\n",
    "                    'model', 'dataset', 'run', \n",
    "                    'num_A_standard_prompt_with_reasoning', 'num_A_newline_prompt_with_reasoning', \n",
    "                    'num_A_standard_prompt_no_reasoning', 'num_A_newline_prompt_no_reasoning',\n",
    "                    'num_other_standard_prompt_with_reasoning', 'num_other_newline_prompt_with_reasoning',\n",
    "                    'num_other_standard_prompt_no_reasoning', 'num_other_newline_prompt_no_reasoning',\n",
    "                    'num_prompt_var_cause_flip_with_reasoning', \n",
    "                    'num_prompt_var_cause_flip_no_reasoning', \n",
    "                    #  TODO RENAME 'num_reasoning_cause_flip'\n",
    "                    'num_reasoning_cause_flip'\n",
    "                    ]\n",
    "                )\n",
    "            \n",
    "            # results_changes = pd.DataFrame(\n",
    "            #     columns=[\n",
    "                    \n",
    "            #         ]\n",
    "            #     )\n",
    "            \n",
    "            # load dataset\n",
    "            standard_with_reasoning_path = f'mceval_{run}/{model_identifier}/scores_with_reasoning_{dataset}_standard.jsonl'\n",
    "            standard_no_reasoning_path = f'mceval_{run}/{model_identifier}/scores_without_reasoning_{dataset}_standard.jsonl'\n",
    "            newline_with_reasoning_path = f'mceval_{run}/{model_identifier}/scores_with_reasoning_{dataset}_newline.jsonl'\n",
    "            newline_no_reasoning_path = f'mceval_{run}/{model_identifier}/scores_without_reasoning_{dataset}_newline.jsonl'\n",
    "            # load as df\n",
    "            standard_with_reasoning = pd.read_json(standard_with_reasoning_path, lines=True, orient='records')\n",
    "            standard_no_reasoning = pd.read_json(standard_no_reasoning_path, lines=True, orient='records')\n",
    "            newline_with_reasoning = pd.read_json(newline_with_reasoning_path, lines=True, orient='records')\n",
    "            newline_no_reasoning = pd.read_json(newline_no_reasoning_path, lines=True, orient='records')\n",
    "            \n",
    "            id_colname = 'top_k_ids'\n",
    "            # top_id_is_A and other\n",
    "            standard_with_reasoning['top_id_is_A'] = standard_with_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            standard_no_reasoning['top_id_is_A'] = standard_no_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            standard_with_reasoning['top_id_is_other'] = standard_with_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            standard_no_reasoning['top_id_is_other'] = standard_no_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            \n",
    "            newline_with_reasoning['top_id_is_A'] = newline_with_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            newline_no_reasoning['top_id_is_A'] = newline_no_reasoning[id_colname].apply(lambda x: x[0] == token_A)\n",
    "            newline_with_reasoning['top_id_is_other'] = newline_with_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            newline_no_reasoning['top_id_is_other'] = newline_no_reasoning[id_colname].apply(lambda x: x[0] != token_A and x[0] != token_B)\n",
    "            # count number of token_A \n",
    "            results['num_A_standard_prompt_with_reasoning'] = standard_with_reasoning['top_id_is_A'].value_counts().get(True, 0)\n",
    "            results['num_A_standard_prompt_no_reasoning'] = standard_no_reasoning['top_id_is_A'].value_counts().get(True, 0)\n",
    "            newline_num_A_with_reasoning = newline_with_reasoning['top_id_is_A'].value_counts().get(True, 0)\n",
    "            newline_num_A_no_reasoning = newline_no_reasoning['top_id_is_A'].value_counts().get(True, 0)\n",
    "            # count number of other\n",
    "            standard_num_other_with_reasoning = standard_with_reasoning['top_id_is_other'].value_counts().get(True, 0)\n",
    "            standard_num_other_no_reasoning = standard_no_reasoning['top_id_is_other'].value_counts().get(True, 0)\n",
    "            newline_num_other_with_reasoning = newline_with_reasoning['top_id_is_other'].value_counts().get(True, 0)\n",
    "            newline_num_other_no_reasoning = newline_no_reasoning['top_id_is_other'].value_counts().get(True, 0)\n",
    "            # num diff in top_id_is_A (between with and without reasoning)\n",
    "            other_comparisons = pd.DataFrame()\n",
    "            other_comparisons['standard_flipped'] = standard_with_reasoning['top_id_is_A'] != standard_no_reasoning['top_id_is_A']\n",
    "            other_comparisons['newline_flipped'] = newline_with_reasoning['top_id_is_A'] != newline_no_reasoning['top_id_is_A']\n",
    "            # num diff in top_id_is_A (between standard and newline)\n",
    "            other_comparisons['prompt_var_flip_with_reasoning'] = standard_with_reasoning['top_id_is_A'] != newline_with_reasoning['top_id_is_A']\n",
    "            other_comparisons['prompt_var_flip_no_reasoning'] = standard_no_reasoning['top_id_is_A'] != newline_no_reasoning['top_id_is_A']\n",
    "            other_comparisons['prompt_var_flip_to_no_flip'] = other_comparisons['standard_flipped'] != other_comparisons['newline_flipped']\n",
    "            #TODO add values to results df\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, dataset, prompt_var):\n",
    "    scores_without_reasoning_path = f'{results_path}/{model}/{dataset}/scores_without_reasoning_{prompt_var}.jsonl'\n",
    "    scores_with_reasoning_path = f'{results_path}/{model}/{dataset}/scores_with_reasoning_{prompt_var}.jsonl'\n",
    "    scores_without_reasoning = pd.read_json(scores_without_reasoning_path, orient='records', lines=True)\n",
    "    scores_with_reasoning = pd.read_json(scores_with_reasoning_path, orient='records', lines=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['top_k_scores', 'top_k_ids'], dtype='object')\n",
      "Index(['top_k_scores', 'top_k_ids'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print columns \n",
    "print(scores_without_reasoning.columns)\n",
    "print(scores_with_reasoning.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redteam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
